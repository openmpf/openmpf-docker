#############################################################################
# NOTICE                                                                    #
#                                                                           #
# This software (or technical data) was produced for the U.S. Government    #
# under contract, and is subject to the Rights in Data-General Clause       #
# 52.227-14, Alt. IV (DEC 2007).                                            #
#                                                                           #
# Copyright 2024 The MITRE Corporation. All Rights Reserved.                #
#############################################################################

#############################################################################
# Copyright 2024 The MITRE Corporation                                      #
#                                                                           #
# Licensed under the Apache License, Version 2.0 (the "License");           #
# you may not use this file except in compliance with the License.          #
# You may obtain a copy of the License at                                   #
#                                                                           #
#    http://www.apache.org/licenses/LICENSE-2.0                             #
#                                                                           #
# Unless required by applicable law or agreed to in writing, software       #
# distributed under the License is distributed on an "AS IS" BASIS,         #
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  #
# See the License for the specific language governing permissions and       #
# limitations under the License.                                            #
#############################################################################

# Use this file in conjunction with docker-compose.core.yml.

version: '3.7'

x-detection-component-base:
  &detection-component-base
  environment: &common-env-vars
    WFM_USER:
    WFM_PASSWORD:
    ACTIVE_MQ_BROKER_URI:
  depends_on:
    - workflow-manager
  volumes:
    - shared_data:/opt/mpf/share
  deploy: &common-deploy
    mode: replicated
    replicas: 2


services:
  argos_translation:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_argos_translation:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/ArgosTranslation

  azure_form_detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_azure_form_detection:${TAG}
    build:
      context: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/AzureFormDetection
    environment:
      <<: *common-env-vars
      MPF_PROP_ACS_URL: ${ACS_FORM_DETECTION_URL}
      MPF_PROP_ACS_SUBSCRIPTION_KEY: ${ACS_FORM_DETECTION_SUBSCRIPTION_KEY}

  azure_ocr_text_detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_azure_ocr_text_detection:${TAG}
    build:
      context: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/AzureOcrTextDetection
    environment:
      <<: *common-env-vars
      MPF_PROP_ACS_URL: ${ACS_OCR_URL}
      MPF_PROP_ACS_SUBSCRIPTION_KEY: ${ACS_OCR_SUBSCRIPTION_KEY}

  azure_read_text_detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_azure_read_text_detection:${TAG}
    build:
      context: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/AzureReadTextDetection
    environment:
      <<: *common-env-vars
      MPF_PROP_ACS_URL: ${ACS_READ_URL}
      MPF_PROP_ACS_SUBSCRIPTION_KEY: ${ACS_READ_SUBSCRIPTION_KEY}

  azure_speech_detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_azure_speech_detection:${TAG}
    build:
      context: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/AzureSpeechDetection
    environment:
      <<: *common-env-vars
      MPF_PROP_ACS_URL: ${ACS_SPEECH_URL}
      MPF_PROP_ACS_SUBSCRIPTION_KEY: ${ACS_SPEECH_SUBSCRIPTION_KEY}
      MPF_PROP_ACS_BLOB_CONTAINER_URL: ${ACS_SPEECH_BLOB_CONTAINER_URL}
      MPF_PROP_ACS_BLOB_SERVICE_KEY: ${ACS_SPEECH_BLOB_SERVICE_KEY}

  azure_translation:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_azure_translation:${TAG}
    build:
      context: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/AzureTranslation
    environment:
      <<: *common-env-vars
      # The URL should not end with /translate because two separate endpoints are used.
      # MPF_PROP_ACS_URL + '/translate' is used for translation and
      # MPF_PROP_ACS_URL + '/breaksentence' is used to break up long text.
      MPF_PROP_ACS_URL: ${ACS_TRANSLATION_URL}
      MPF_PROP_ACS_SUBSCRIPTION_KEY: ${ACS_TRANSLATION_SUBSCRIPTION_KEY}

  clip-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_clip_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/ClipDetection

  clip-detection-server:
    image: ${REGISTRY}openmpf_clip_detection_server:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/ClipDetection/triton_server
    deploy:
      mode: global
    ulimits:
      memlock: -1  # don't limit locked-in memory (prevent paging)
      stack: 67108864  # 8 GiB
    # ports:
      # - "8001:8001"  # server gRPC port (expose to enable handling requests from outside the stack)
      # - "8000:8000"  # (optional) HTTP management port
      # - "8002:8002"  # (optional) Prometheus metrics at http://<host>:<this-port>/metrics
    environment:
      # Optionally, limit the GPUs exposed to the server.
      # At least one GPU is required to run the YOLO TensorRT engine.
      - NVIDIA_VISIBLE_DEVICES=all

    command: [tritonserver,
              --model-repository=/models,
              --strict-model-config=false,
              --model-control-mode=explicit,
              --load-model=vit_l_14,
              # --log-verbose=1,  # (optional)
              --grpc-infer-allocation-pool-size=16 ]

  east-text-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_east_text_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/EastTextDetection
    deploy:
      <<: *common-deploy
      resources:
        limits:
          cpus: "2.0"

  keyword-tagging:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_keyword_tagging:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/KeywordTagging

  mog-motion-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_mog_motion_detection:${TAG}
    build:
      context: ${OPENMPF_PROJECTS_PATH}/openmpf-contrib-components/cpp/motion
      dockerfile: MogMotionDetection/Dockerfile

  nlp-text-correction:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_nlp_text_correction:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/NlpTextCorrection

  oalpr-license-plate-text-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_oalpr_license_plate_text_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/OalprLicensePlateTextDetection

  ocv-dnn-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_ocv_dnn_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/OcvDnnDetection

  ocv-face-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_ocv_face_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/OcvFaceDetection

  ocv-yolo-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_ocv_yolo_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/OcvYoloDetection

  ocv-yolo-detection-server:
    image: ${REGISTRY}openmpf_ocv_yolo_detection_server:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/OcvYoloDetection/triton_server
    deploy:
      mode: global
    ulimits:
      memlock: -1  # don't limit locked-in memory (prevent paging)
      stack: 67108864  # 8 GiB
    # ports:
    #  - "8001:8001"  # server gRPC port (expose to enable handling requests from outside the stack)
    #  - "8000:8000"  # (optional) HTTP management port
    #  - "8002:8002"  # (optional) Prometheus metrics at http://<host>:<this-port>/metrics
    environment:
      # Optionally, limit the GPUs exposed to the server.
      # At least one GPU is required to run the YOLO TensorRT engine.
      - NVIDIA_VISIBLE_DEVICES=all
      - LD_PRELOAD=/plugins/libyolo608layerplugin.so
    volumes:
      # (optional) Volume for shared memory with OcvYoloDetection component on same host
      # - "/dev/shm:/dev/shm"
      # At runtime the server generates a GPU-specific engine file for the YOLO model. Caching it in a volume saves
      # initialization time. To save space, manually remove generated engine files that are no longer needed.
      - yolo_engine_files:/models
    command: [tritonserver,
              --model-repository=/models,
              --strict-model-config=false,
              --model-control-mode=explicit,
              --load-model=yolo-608,
              # --log-verbose=1,  # (optional)
              --grpc-infer-allocation-pool-size=16]

  scene-change-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_scene_change_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/SceneChangeDetection

  sphinx-speech-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_sphinx_speech_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/java/SphinxSpeechDetection
    environment:
      <<: *common-env-vars
      JAVA_TOOL_OPTIONS: -Xmx1g  # limit Java heap size to 1GB

  subsense-motion-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_subsense_motion_detection:${TAG}
    build:
      context: ${OPENMPF_PROJECTS_PATH}/openmpf-contrib-components/cpp/motion
      dockerfile: SubsenseMotionDetection/Dockerfile

  tesseract-ocr-text-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_tesseract_ocr_text_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/TesseractOCRTextDetection

  tika-image-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_tika_image_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/java/TikaImageDetection

  tika-text-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_tika_text_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/java/TikaTextDetection

  transformer-tagging:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_transformer_tagging:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/TransformerTagging

  trtis-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_trtis_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/TrtisDetection
    environment:
      <<: *common-env-vars
      # Specifies container and port of TRTIS server.
      MPF_PROP_TRTIS_SERVER: trtis-detection-server:8001
    depends_on:
      - workflow-manager
      - trtis-detection-server

  trtis-detection-server:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_trtis_detection_server:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/TrtisDetection/trtserver_dockerfile
    deploy:
      mode: global
    # ports:
    #  - "8002:8002"  # (optional) Prometheus metrics at http://<host>:<this-port>/metrics
    environment:
      # Use "NVIDIA_VISIBLE_DEVICES=" (no value) for CPU mode.
      # Set to "all" to use all available GPUs.
      - NVIDIA_VISIBLE_DEVICES=
    # Uncomment to attach custom models.
    # Currently server is prebuilt with the coco model.
    #volumes:
    #  - "${OPENMPF_PROJECTS_PATH}/openmpf-components/cpp/TrtisDetection/plugin-files/models:/models"
    command: [trtserver,--model-store=/models]

  whisper-speech-detection:
    <<: *detection-component-base
    image: ${REGISTRY}openmpf_whisper_speech_detection:${TAG}
    build: ${OPENMPF_PROJECTS_PATH}/openmpf-components/python/WhisperSpeechDetection

volumes:
  yolo_engine_files:
